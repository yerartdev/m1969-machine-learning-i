{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2 - Redes neuronales con KERAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grupo de Meteorología"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué es KERAS?\n",
    "\n",
    "KERAS es una librería diseñada por el francés François Chollet, ingeniero de Google, que permite diseñar de manera sencilla una amplia gama de arquitecturas de redes neuronales basadas en los recientes avances en el campo de deep learning. KERAS se apoya en otras librerías de diseño de redes neuronales, como TensorFlow o Theano, pero su diferencia radica en la facilidad para el usuario de utilizar KERAS en comparación con las otras. Dicho de otro modo, KERAS permite de una manera sencilla y amigable para el usuario, utilizar librerías potentes en el ámbito del deep learning. Esta facilidad viene de la mano de un diseño modular. Como veremos, KERAS define las capas como objetos con ciertas propiedades y por tanto una red se construye como una secuencia de objetos o capas. Además, KERAS permite implementar de manera sencilla los avances de deep learning: redes convolucionales, redes recurrentes,funciones de activación ReLu, stochastic gradient descent …\n",
    "\n",
    "Las ventajas de KERAS se resumen en:\n",
    "\n",
    "- El código se puede correr tanto en la CPU como en GPU.\n",
    "- Permite implementar de manera sencilla y rápida estructuras de deep learning gracias a su estructura secuencial de capas.\n",
    "- Se apoya en librerías de deep learning: TensorFlow, CNTK y Theano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo instalar KERAS?\n",
    "\n",
    "Recientemente, KERAS se encuentra en el repositorio de CRAN. Por ello, su instalación se realiza mediante el sistema de instalación de paquetes visto hasta ahora. Sin embargo, como KERAS se apoya en TensorFlow, para completar la instalación hay que recurrir a la función “install_keras()” de la librería keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install.packages(\"keras\")\n",
    "# library(keras)\n",
    "# install_keras()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo construir una red neuronal en KERAS?\n",
    "\n",
    "Para guiarnos durante la elaboración de nuestra primera red neuronal en KERAS vamos a usar el dataset MNIST. Recordamos que el MNIST cuenta con 70.000 imágenes 28x28 pixeles, siendo 60.000 para entrenar y 10.000 utilizadas como test. La librería KERAS ya cuenta internamente con el dataset MNIST por lo que no es necesario descargarlo de cualquier otro repositorio de datos como Kaggle. Por tanto, para cargar los datos mediante KERAS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ImportError: No module named keras\n",
      "Use the install_keras() function to install the core Keras library\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error: Error loading Python module keras\n",
     "output_type": "error",
     "traceback": [
      "Error: Error loading Python module keras\nTraceback:\n",
      "1. dataset_mnist()",
      "2. keras$datasets",
      "3. `$.python.builtin.module`(keras, datasets)",
      "4. py_resolve_module_proxy(x)",
      "5. stop(\"Error loading Python module \", module, call. = FALSE)"
     ]
    }
   ],
   "source": [
    "mnist <- dataset_mnist()\n",
    "x_train <- mnist$train$x\n",
    "y_train <- mnist$train$y\n",
    "x_test <- mnist$test$x\n",
    "y_test <- mnist$test$y\n",
    "str(x_train)\n",
    "\n",
    "##  int [1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos como el x_train es una array de 3 dimensiones, donde la primera dimensión es la observación/imagen y las 2 siguientes la posición del pixel. Ahora bien, para tener la estructura de las redes neuronales vistas hasta ahora, los predictores (o pixeles en este caso) representan una única capa y por tanto es más adecuado concatenar los predictores, recreando la estructura de capa de entrada de una red neuronal tradicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(x_train) <- c(nrow(x_train), 784)\n",
    "dim(x_test) <- c(nrow(x_test), 784)\n",
    "str(x_train)\n",
    "\n",
    "##  int [1:60000, 1:784] 0 0 0 0 0 0 0 0 0 0 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EL resultado de la concatenación es esta vez una matrix de dos dimensiones, donde la primera dimensión representa la observación/imagen y la siguiente los 28 * 28 = 784 pixeles. Por otra parte, también hay que reescalar los datos para que no haya problemas de escala entre predictores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train <- x_train / 255\n",
    "x_test <- x_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez preprocesada la capa de entrada, constituida por x_train o x_test, hay que darle a y_train/y_test la estructura de capa de salida que deseemos. Es decir, la pregunta que debemos hacernos es, ¿cuántas neuronas de salida va a tener mi red? Una manera de abordar el problema (y la que se usa en la actualidad) es introducir tantas neuronas de salida como clases a identificar. En este caso pretendemos identificar 10 clases: si el número es un 0, o un 1, o un 2, …. o un 9. Por tanto, tendremos 10 neuronas en la capa de salida, siendo cada una de ellas la responsable de identificar una de las 10 clases en concreto. Por ejemplo: la neurona de salida número 1 es la encargada de identificar si el número que “ve” la red es un 0 o no lo es; la neurona de salida número 2 es la encargada de identificar si el número que “ve” la red es un 1 o no lo es, …, y así sucesivamente hasta 10 neuronas.\n",
    "\n",
    "Para que nuestros datos tengan estructura de 10 neuronas ocultas hay que redimensionar nuestros datos de manera que se convierta en una matriz de dos dimensiones: donde la primera dimensión es la observación y la segunda la neurona oculta. Afortunadamente, KERAS tiene predefinida esta función que es capaz de redimensionar los datos por categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(y_train)\n",
    "\n",
    "## [1] 5 0 4 1 9 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train <- to_categorical(y_train, 10)\n",
    "head(y_train)\n",
    "\n",
    "##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n",
    "## [1,]    0    0    0    0    0    1    0    0    0     0\n",
    "## [2,]    1    0    0    0    0    0    0    0    0     0\n",
    "## [3,]    0    0    0    0    1    0    0    0    0     0\n",
    "## [4,]    0    1    0    0    0    0    0    0    0     0\n",
    "## [5,]    0    0    0    0    0    0    0    0    0     1\n",
    "## [6,]    0    0    1    0    0    0    0    0    0     0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como la primera imagen que era un número 5, se ha convertido a binario: ahora es un 1 en la columna 6 (es decir, la sexta neurona oculta) y 0 en las demás (ya que no es ningún otro número que no sea un 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que ya están preparados los datasets, podemos proceder a construir nuestro modelo. La construcción de un modelo mediante esta librería se estructura en una serie de pasos:\n",
    "\n",
    "- Definimos como va a interactuar el usuario con KERAS: de manera secuencial o de manera funcional: En el primer caso la red es una secuencia lineal de capas, mientras que el segundo caso permite una elaboración de redes con topologías más complejas (como por ejemplo redes acícliclas). En este curso tan solo vamos a ver la manera secuencial de interactuar con KERAS.\n",
    "- Definimos la estructura de la red: tipo de capa (dense, convolutional, …), número de capas, número de neuronas ocultas, función de activación, penalización de los pesos …\n",
    "- Compilamos el modelo: en esta parte se introduce la función de coste a minimizar, el algoritmo de aprendizaje (backpropagation, backpropagation + momentum, …).\n",
    "- Se ajusta el modelo, es decir se entrena. KERAS proporciona herramientas de visualización del error de ajuste.\n",
    "\n",
    "De acuerdo con estos pasos vamos a elaborar una red neuronal de dos capas con función de activación sigmoidal tanto en las capas ocultas como en la capa de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1. Definir nuestro modelo como una estructura secuencial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 <- keras_model_sequential() \n",
    "str(model1)\n",
    "\n",
    "## Model\n",
    "## ___________________________________________________________________________\n",
    "## Layer (type)                     Output Shape                  Param #     \n",
    "## ===========================================================================\n",
    "## Total params: 0\n",
    "## Trainable params: 0\n",
    "## Non-trainable params: 0\n",
    "## ___________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2. Definimos la topología de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 %>% \n",
    "  layer_dense(units = 100, input_shape = 784, activation = \"sigmoid\") %>% # Primera capa oculta\n",
    "  layer_dense(units = 100, activation = \"sigmoid\") %>% # Segunda capa oculta\n",
    "  layer_dense(units = 10 , activation = \"sigmoid\") \n",
    "str(model1)\n",
    "\n",
    "## Model\n",
    "## ___________________________________________________________________________\n",
    "## Layer (type)                     Output Shape                  Param #     \n",
    "## ===========================================================================\n",
    "## dense_1 (Dense)                  (None, 100)                   78500       \n",
    "## ___________________________________________________________________________\n",
    "## dense_2 (Dense)                  (None, 100)                   10100       \n",
    "## ___________________________________________________________________________\n",
    "## dense_3 (Dense)                  (None, 10)                    1010        \n",
    "## ===========================================================================\n",
    "## Total params: 89,610\n",
    "## Trainable params: 89,610\n",
    "## Non-trainable params: 0\n",
    "## ___________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar como hemos definido una red con 3 capas (la capa de entrada no cuenta como capa), teniendo las 2 capas ocultas 100 neuronas ocultas y la última capa es la de salida con 10 neuronas ocultas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3. Definir cómo va a aprender la red: algoritmo de aprendizaje y función a minimizar. \n",
    "\n",
    "KERAS ya tiene incluidos una gran variedad de algoritmos de aprendizaje (“rmsprop” or “adagrad”, “backpropagation”, …), así como funciones objetivo (“mean square error”, “root mean square error”, …). Más información sobre los distintos tipos de algoritmos de aprendizaje y de las funciones objetivo prediseñadas que tiene KERAS se pueden encontrar en el siguiente link: . Nosotros en este caso vamos a entrenar utilizando lo visto hasta ahora: minimizar el mean square error mediante gradient descent utilizando el algoritmo de backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 %>% compile(\n",
    "  optimizer = optimizer_sgd(lr = 0.1),\n",
    "  # optimizer = optimizer_sgd(lr = 0.001, momentum = 0.001), # si quisiésemos backpropagation con momento.\n",
    "  loss = \"mse\",\n",
    "  metrics = \"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 4. Entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history <- model1 %>% fit(x_train, y_train, epochs = 100, batch_size = 100)\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la figura anterior podemos ver la evolución del error (en este caso el mean square error) y la evolución de la métrica (en este caso el accuracy), por época. Se observa como el error disminuye en cada época y el accuracy aumenta, como es de esperar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar nuestro modelo en un conjunto nuevo de datos se utiliza la manera tradicional, es decir, utilizando la función predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred <- predict(model1,x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque KERAS también tiene dos funciones propias para predecir en un nuevo conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(x_test,y_test) # Te devuelve la métrica y el error (función objetivo) en el nuevo dataset.\n",
    "pred <- predict_classes(model1,x_test)\n",
    "rbind(y_test,pred)[2,1:10]\n",
    "\n",
    "##  [1] 7 2 1 0 4 1 4 9 6 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo saber si nuestra red sobreajusta y cómo evitarlo?\n",
    "\n",
    "El gran número de parámetros a ajustar en una red neuronal las hace propensas a sobreajustar si el número de datos no es suficiente. Además, mediante gradient descent nos movemos en la dirección que minimiza el error en los datos de train pero que no tiene por qué ser la misma dirección que en otra muestra de la misma población. Para controlar si el error difiere entre distintas muestras de la misma población, se suele dividir el dataset de train en un dataset A, que es el que se utiliza para entrenar, y un dataset B, que es el que se usa como dataset de control. Para controlar esta divergencia en los errores se dibuja el error estimado por época, de manera que se pueda observar en qué momento del aprendizaje la red empieza a ajustar. La separación del dataset de train en dos (A y B) se puede realizar directamente a la hora de ajustar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 <- keras_model_sequential() \n",
    "model2 %>% \n",
    "  layer_dense(units = 100, input_shape = 784, activation = \"sigmoid\") %>% # Primera capa oculta\n",
    "  layer_dense(units = 100, activation = \"sigmoid\") %>% # Segunda capa oculta\n",
    "  layer_dense(units = 10 , activation = \"sigmoid\") \n",
    "\n",
    "model2 %>% compile(\n",
    "  optimizer = optimizer_sgd(lr = 0.1),\n",
    "  # optimizer = optimizer_sgd(lr = 0.001, momentum = 0.001), # si quisiésemos backpropagation con momento.\n",
    "  loss = \"mse\",\n",
    "  metrics = \"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history <- model2 %>% fit(x_train, y_train, epochs = 100, batch_size = 100, validation_split = 0.2)\n",
    "# en este caso el 20% de los datos de train se van a utilizar como dataset de control y NUNCA se van a usar para entrenar.\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo obtener información sobre el modelo inferido?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparte de calcular las predicciones, uno puede querer saber cuales han sido los valores de los coeficientes estimados así como los gradientes. Una situación en la que uno puede estar interesado en esta información es, por ejemplo, cuando regulariza el modelo mediante la norma L1. Recordamos que este tipo de regularización hace 0 algunos de los coeficientes, generalmente aquellos que conectan con neuronas irrelevantes para la predicción en cuestión, limitando la complejidad del modelo. Puede ser interesante, en el caso de las imágenes, ver que pixeles son los más relevantes a la hora de determinar qué número infiere el modelo que está viendo. Por ello, en esta sección vamos a 1) aprender a introducir este tipo de penalización en las redes neuronales mediante KERAS y 2) aprender a obtener los coeficientes del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para introducir este tipo de penalización hay que especificarlo a la hora de definir la topología de la red neuronal. Para ver la penalización sobre el campo original (capa de entrada), vamos a realizar una regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularizacion L1\n",
    "model3 <- keras_model_sequential() \n",
    "model3 %>% \n",
    "  layer_dense(units = 10, input_shape = 784, activation = \"sigmoid\", kernel_regularizer = regularizer_l1(0.0001))\n",
    "\n",
    "model3 %>% compile(\n",
    "  optimizer = optimizer_sgd(lr = 0.5, momentum = 0.5),\n",
    "  loss = \"mse\",\n",
    "  metrics = \"accuracy\"\n",
    ")\n",
    "history3 <- model3 %>% fit(x_train, y_train, epochs = 20, batch_size = 100, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularizacion L2\n",
    "model4 <- keras_model_sequential() \n",
    "model4 %>% \n",
    "  layer_dense(units = 10, input_shape = 784, activation = \"sigmoid\", kernel_regularizer = regularizer_l2(0.0001))\n",
    "\n",
    "model4 %>% compile(\n",
    "  optimizer = optimizer_sgd(lr = 0.5, momentum = 0.5),\n",
    "  loss = \"mse\",\n",
    "  metrics = \"accuracy\"\n",
    ")\n",
    "history4 <- model4 %>% fit(x_train, y_train, epochs = 20, batch_size = 100, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow = c(1,2))\n",
    "plot(history3); plot(history4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver qué coeficientes ha estimado se hace lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsL1 <- get_layer(object = model3, index = 1L)$get_weights()[[1]] \n",
    "biasL1 <- get_layer(object = model3, index = 1L)$get_weights()[[2]]\n",
    "str(weightsL1)\n",
    "\n",
    "##  num [1:784, 1:10] 2.86e-05 3.14e-05 3.35e-05 -4.43e-05 2.58e-05 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsL2 <- get_layer(object = model4, index = 1L)$get_weights()[[1]] \n",
    "biasL2 <- get_layer(object = model4, index = 1L)$get_weights()[[2]]\n",
    "str(weightsL2)\n",
    "\n",
    "##  num [1:784, 1:10] 0.007633 0.00744 -0.012589 -0.007045 0.000866 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar los pesos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(weightsL1) <- c(28,28,10)\n",
    "dim(weightsL2) <- c(28,28,10)\n",
    "par(mfrow = c(2,5))\n",
    "image(weightsL1[,,1], col = gray.colors(10, start = 0, end = 1))\n",
    "image(weightsL1[,,2], col = gray.colors(10, start = 0, end = 1))\n",
    "image(weightsL1[,,3], col = gray.colors(10, start = 0, end = 1))\n",
    "image(weightsL1[,,4], col = gray.colors(10, start = 0, end = 1))\n",
    "image(weightsL1[,,5], col = gray.colors(10, start = 0, end = 1))\n",
    "image(weightsL2[,,1], col = gray.colors(10, start = 0, end = 1))\n",
    "image(weightsL2[,,2], col = gray.colors(10, start = 0, end = 1))\n",
    "image(weightsL2[,,3], col = gray.colors(10, start = 0, end = 1))\n",
    "image(weightsL2[,,4], col = gray.colors(10, start = 0, end = 1))\n",
    "image(weightsL2[,,5], col = gray.colors(10, start = 0, end = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cuál es el beneficio entre usar KERAS y una librería más tradicional de redes neuronales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparte de los beneficios relacionados con las técnicas de deep learning que son accesibles mediante la librería KERAS, hay otro beneficio importante con respecto a las librerías tradicionales: la velocidad de computo. En anteriores prácticas vimos como diseñar redes neuronales mediante la librería RSNNS. En esta sección vamos a comparar la velocidad de realizar una época para una misma configuración (número de capas y de neuronas) en ambas librerías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(RSNNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeRSNNS_begin <- Sys.time() \n",
    "model_RSNNS <- mlp(x_train,y_train, linOut = FALSE,\n",
    "                   learnFunc = \"Std_Backpropagation\", learnFuncParams = c(0.2),\n",
    "                   size = c(10),\n",
    "                   maxit = 1)\n",
    "timeRSNNS_end <- Sys.time()\n",
    "timeRSNNS <- timeRSNNS_end - timeRSNNS_begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_KERAS <- keras_model_sequential() \n",
    "model_KERAS %>% \n",
    "  layer_dense(units = 10, input_shape = 784, activation = \"sigmoid\") %>% # Capa oculta \n",
    "  layer_dense(units = 10, activation = \"sigmoid\") # Capa de salida\n",
    "model_KERAS %>% compile(\n",
    "  optimizer = optimizer_sgd(lr = 0.2),\n",
    "  loss = \"mse\"\n",
    ")\n",
    "timeKERAS_begin <- Sys.time() \n",
    "model_KERAS %>% fit(x_train, y_train, epochs = 1, batch_size = 60000)\n",
    "timeKERAS_end <- Sys.time()\n",
    "timeKERAS <- timeKERAS_end - timeKERAS_begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeRSNNS\n",
    "\n",
    "## Time difference of 10.87322 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeKERAS\n",
    "\n",
    "## Time difference of 0.6284597 secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pero aun hay más …\n",
    "\n",
    "En esta práctica hemos visto como diseñar redes neuronales tradicionales pero con una librería más potente, como es KERAS. Hemos visto que esta librería, aparte de versatil en el diseño de las redes es bastante rápida. En un simple ejemplo hemos visto como para una misma configuración de red, la estimación de los parámetros era 60 veces más rápido que con una librería tradicional!!!\n",
    "\n",
    "En cuanto al diseño de las redes hemos visto las capas densas (las que están totalmente conectadas con la siguiente capa y hacia delante). Sin embargo, también hay objetos para capas convolucionales y recurrentes, por lo que la implementación de este tipo de capas (complejas a nivel de programación a simple vista) también resulta sencilla. Veremos más en siguientes prácticas pero si alguien está interesado el link a la página de KERAS es este: https://keras.io/ y la página de wikipedia: https://en.wikipedia.org/wiki/Keras.\n",
    "\n",
    "Un aspecto importante de KERAS es que es una librería diseñada para python. Por tanto, a nivel de código interno, el lenguage de programación es python y no R. Sin embargo hay una interfaz de R que nos permite utilizar KERAS en RStudio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
